# Databricks notebook source
# DBTITLE 1,Setup Connection to MDWDataLake Storage Account
spark.conf.set(
  "fs.azure.account.key.mdwdatalake53a5g.blob.core.windows.net",
  "cOM5LdZNV/n70wwHLsgGtUioVknpVbUlpXX3ey8AUUfrh6u32zSErB0nmupbCvm9E0viPmfbtyDLvzOeUCyffA==")

# COMMAND ----------

# DBTITLE 1,Define NYCTaxiData schema and load data into dataframe
from pyspark.sql.types import *

nycTaxiDataSchema = StructType([
  StructField("VendorID",IntegerType(),True)
  , StructField("tpep_pickup_datetime",DateType(),True)
  , StructField("tpep_dropoff_datetime",DateType(),True)
  , StructField("passenger_count",IntegerType(),True)
  , StructField("trip_distance",DoubleType(),True)
  , StructField("RatecodeID",IntegerType(),True)
  , StructField("store_and_fwd_flag",StringType(),True)
  , StructField("PULocationID",IntegerType(),True)
  , StructField("DOLocationID",IntegerType(),True)
  , StructField("payment_type",IntegerType(),True)
  , StructField("fare_amount",DoubleType(),True)
  , StructField("extra",DoubleType(),True)
  , StructField("mta_tax",DoubleType(),True)
  , StructField("tip_amount",DoubleType(),True)
  , StructField("tolls_amount",DoubleType(),True)
  , StructField("improvement_surcharge",DoubleType(),True)
  , StructField("total_amount",DoubleType(),True)])
  
dfNYCTaxiData = spark.read.format('csv').options(header='true', schema=nycTaxiDataSchema).load('wasbs://nyctaxidata@mdwdatalake53a5g.blob.core.windows.net/')

# COMMAND ----------

# DBTITLE 1,Display Data Frame Content
display(dfNYCTaxiData)

# COMMAND ----------

# DBTITLE 1,Use Data Frame Operations to Filter Data
display(dfNYCTaxiData.select("tpep_pickup_datetime", "passenger_count", "total_amount").filter("passenger_count > 6 and total_amount > 50.0"))

# COMMAND ----------

# DBTITLE 1,Create Temp View
dfNYCTaxiData.createOrReplaceTempView('NYCTaxiDataTable')

# COMMAND ----------

# DBTITLE 1,Use SQL to count NYC Taxi Data Records
# MAGIC %sql
# MAGIC select count(*) from NYCTaxiDataTable

# COMMAND ----------

# DBTITLE 1,Use SQL to filter NYC taxi Data records
# MAGIC %sql
# MAGIC 
# MAGIC select cast(tpep_pickup_datetime as date) as pickup_date
# MAGIC   , tpep_dropoff_datetime
# MAGIC   , passenger_count
# MAGIC   , total_amount
# MAGIC from NYCTaxiDataTable
# MAGIC where cast(tpep_pickup_datetime as date) = '2019-04-07'
# MAGIC   and passenger_count > 5

# COMMAND ----------

# DBTITLE 1,Use SQL to aggregate NYC Taxi Data records and visualize data
# MAGIC %sql
# MAGIC 
# MAGIC select case payment_type
# MAGIC             when 1 then 'Credit card'
# MAGIC             when 2 then 'Cash'
# MAGIC             when 3 then 'No charge'
# MAGIC             when 4 then 'Dispute'
# MAGIC             when 5 then 'Unknown'
# MAGIC             when 6 then 'Voided trip'
# MAGIC         end as PaymentType
# MAGIC   , count(*) as TotalRideCount
# MAGIC from NYCTaxiDataTable
# MAGIC group by payment_type
# MAGIC order by TotalRideCount desc

# COMMAND ----------

# DBTITLE 1,Load Taxi Location Data from Azure SQL Data Warehouse
jdbcUrl = "jdbc:sqlserver://mdwsqlvirtualserver-53a5g.database.windows.net:1433;database=MDWASQLDW"
connectionProperties = {
  "user" : "mdwadmin",
  "password" : "P@ssw0rd123!",
  "driver" : "com.microsoft.sqlserver.jdbc.SQLServerDriver"
}

pushdown_query = '(select * from NYC.TaxiLocationLookup) as t'
dfLookupLocation = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)

dfLookupLocation.createOrReplaceTempView('NYCTaxiLocation')

display(dfLookupLocation) 

# COMMAND ----------

# DBTITLE 1,Combine Data Lake and Data Warehouse data frames using SQL
# MAGIC %sql
# MAGIC 
# MAGIC select 
# MAGIC     pu.Borough
# MAGIC   , cast(tpep_pickup_datetime as date) as pickup_date
# MAGIC   , passenger_count
# MAGIC   , total_amount
# MAGIC from NYCTaxiDataTable as rides
# MAGIC   join NYCTaxiLocation as pu
# MAGIC     on rides.PULocationID = pu.LocationID
# MAGIC where cast(tpep_pickup_datetime as date) = '2018-04-07'
# MAGIC   and passenger_count > 5
# MAGIC   and total_amount > 50.0

# COMMAND ----------

